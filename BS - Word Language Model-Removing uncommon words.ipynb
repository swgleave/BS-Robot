{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nideas to improve:\\n-train more epochs\\n-more training data\\n-different architecture\\n-different input size\\n-different batch size\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ideas to improve:\n",
    "-train more epochs\n",
    "-more training data\n",
    "-different architecture\n",
    "-different input size\n",
    "-different batch size\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the rumors are true and Doc Rivers gets a contract extension soon, I just hope the Celtics tell the truth in the ensuing news conference. Don't feed us lines like \"Thanks to Doc, our young players \n",
      "['if', 'the', 'rumors', 'are', 'true', 'and', 'doc', 'rivers', 'gets', 'a', 'contract', 'extension', 'soon', 'i', 'just', 'hope', 'the', 'celtics', 'tell', 'the', 'truth', 'in', 'the', 'ensuing', 'news', 'conference', 'dont', 'feed', 'us', 'lines', 'like', 'thanks', 'to', 'doc', 'our', 'young', 'players', 'made', 'major', 'strides', 'this', 'season', 'or', 'we', 'would', 'have', 'made', 'the', 'playoffs', 'if', 'we', 'didnt', 'have', 'so', 'many', 'injuries', 'thats', 'a', 'load', 'of', 'crap', 'maybe', 'those', 'arent', 'lies', 'but', 'they', 'seem', 'like', 'fibs', 'along', 'the', 'lines', 'of', 'i', 'dont', 'think', 'those', 'jeans', 'make', 'you', 'look', 'fat', 'at', 'all', 'theyre', 'just', 'the', 'wrong', 'size', 'and', 'i', 'wasnt', 'checking', 'her', 'out', 'i', 'thought', 'i', 'recognized', 'her', 'am', 'i', 'complaining', 'hell', 'no', 'you', 'couldnt', 'have', 'asked', 'for', 'a', 'better', 'leader', 'for', 'an', 'undercover', 'tanking', 'mission', 'this', 'was', 'like', 'hiring', 'eddie', 'griffin', 'to', 'wreck', 'your', 'archenemys', 'ferrari', 'so', 'why', 'not', 'admit', 'this', 'at', 'the', 'news', 'conference', 'and', 'if', 'youre', 'not', 'admitting', 'it', 'have', 'some', 'fun', 'and', 'hand', 'out', 'a', 'top', 'reasons', 'why', 'were', 'extending', 'doc', 'rivers', 'that', 'have', 'nothing', 'to', 'do', 'with', 'the', 'fact', 'that', 'he', 'successfully', 'executed', 'his', 'tanking', 'mission', 'list', 'reason', 'no', 'its', 'not', 'every', 'day', 'you', 'can', 'lock', 'up', 'a', 'coach', 'with', 'a', 'career', 'record', 'of', 'no', 'we', 'appreciate', 'how', 'doc', 'made', 'the', 'best']\n",
      "Total Tokens: 1016491\n",
      "Unique Tokens: 39395\n",
      "Total Sequences: 1016440\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# load document\n",
    "in_filename = 'cleanedespn.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    "\n",
    "#grab subset\n",
    "#print(len(doc))\n",
    "#doc = doc[:700000]\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'espnword.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WORD_FREQUENCY = 5\n",
    "\n",
    "word_freq = {}\n",
    "for word in tokens:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before ignoring: 39395\n",
      "Ignoring words with frequency < 5\n",
      "Unique words after ignoring: 10798\n"
     ]
    }
   ],
   "source": [
    "words = set(tokens)\n",
    "print('Unique words before ignoring:', len(words))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "words = sorted(set(words) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 644670\n",
      "Remaining sequences: 371796\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LEN = 25\n",
    "STEP = 1\n",
    "\n",
    "sentences = []\n",
    "next_words = []\n",
    "ignored = 0\n",
    "for i in range(0, len(tokens) - SEQUENCE_LEN, STEP):\n",
    "    # Only add the sequences where no word is in ignored_words\n",
    "    if len(set(tokens[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
    "        sentences.append(tokens[i: i + SEQUENCE_LEN])\n",
    "        next_words.append(tokens[i + SEQUENCE_LEN])\n",
    "    else:\n",
    "        ignored = ignored + 1\n",
    "print('Ignored sequences:', ignored)\n",
    "print('Remaining sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t, word_indices[w]] = 1\n",
    "            y[i, word_indices[next_word_list[index % len(sentence_list)]]] = 1\n",
    "            index = index + 1\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dropout=0.2):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128), input_shape=(SEQUENCE_LEN, len(words))))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
    "    seed = (sentences+sentences_test)[seed_index]\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t, word_indices[word]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "\n",
    "            examples_file.write(\" \"+next_word)\n",
    "        examples_file.write('\\n')\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "    examples_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Size of training set = 364360\n",
      "Size of test set = 7436\n"
     ]
    }
   ],
   "source": [
    "(sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(sentences, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % (\n",
    "        len(words),\n",
    "        SEQUENCE_LEN,\n",
    "        MIN_WORD_FREQUENCY\n",
    "    )\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "callbacks_list = [checkpoint, print_callback, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11387/11387 [==============================] - 1220s 107ms/step - loss: 5.1500 - acc: 0.1675 - val_loss: 5.5173 - val_acc: 0.1540\n",
      "Epoch 2/50\n",
      "11387/11387 [==============================] - 1215s 107ms/step - loss: 4.7744 - acc: 0.1948 - val_loss: 5.6011 - val_acc: 0.1553\n",
      "Epoch 3/50\n",
      "11387/11387 [==============================] - 1216s 107ms/step - loss: 4.4011 - acc: 0.2267 - val_loss: 5.7733 - val_acc: 0.1448\n",
      "Epoch 4/50\n",
      "11387/11387 [==============================] - 1216s 107ms/step - loss: 4.0363 - acc: 0.2633 - val_loss: 5.9736 - val_acc: 0.1377\n",
      "Epoch 5/50\n",
      "11387/11387 [==============================] - 1215s 107ms/step - loss: 3.6980 - acc: 0.3036 - val_loss: 6.1341 - val_acc: 0.1293\n",
      "Epoch 6/50\n",
      "11387/11387 [==============================] - 1215s 107ms/step - loss: 3.3779 - acc: 0.3467 - val_loss: 6.3519 - val_acc: 0.1239\n",
      "Epoch 7/50\n",
      " 5042/11387 [============>.................] - ETA: 11:07 - loss: 3.1575 - acc: 0.3768"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "                    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "                    epochs=50,\n",
    "                    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
    "                    validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelv2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 50)            629350    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12587)             1271287   \n",
      "=================================================================\n",
      "Total params: 2,051,537\n",
      "Trainable params: 2,051,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "117185/117185 [==============================] - 147s 1ms/step - loss: 7.2820 - acc: 0.0551\n",
      "Epoch 2/100\n",
      "117185/117185 [==============================] - 143s 1ms/step - loss: 6.9235 - acc: 0.0572\n",
      "Epoch 3/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 6.7480 - acc: 0.0650\n",
      "Epoch 4/100\n",
      "117185/117185 [==============================] - 142s 1ms/step - loss: 6.5891 - acc: 0.0718\n",
      "Epoch 5/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 6.4318 - acc: 0.0813\n",
      "Epoch 6/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 6.3011 - acc: 0.0875\n",
      "Epoch 7/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 6.1881 - acc: 0.0916\n",
      "Epoch 8/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 6.0917 - acc: 0.0951\n",
      "Epoch 9/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 6.0211 - acc: 0.0974\n",
      "Epoch 10/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 5.9298 - acc: 0.1023\n",
      "Epoch 11/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.8445 - acc: 0.1060\n",
      "Epoch 12/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.7634 - acc: 0.1105\n",
      "Epoch 13/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.6868 - acc: 0.1134\n",
      "Epoch 14/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.5987 - acc: 0.1169\n",
      "Epoch 15/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.5244 - acc: 0.1192\n",
      "Epoch 16/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.4380 - acc: 0.1219\n",
      "Epoch 17/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.3655 - acc: 0.1258\n",
      "Epoch 18/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.2915 - acc: 0.1281\n",
      "Epoch 19/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 5.2111 - acc: 0.1305\n",
      "Epoch 20/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.1355 - acc: 0.1330\n",
      "Epoch 21/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.0621 - acc: 0.1359\n",
      "Epoch 22/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.9934 - acc: 0.1388\n",
      "Epoch 23/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.9412 - acc: 0.1416\n",
      "Epoch 24/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.9384 - acc: 0.1412\n",
      "Epoch 25/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.8568 - acc: 0.1454\n",
      "Epoch 26/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.7777 - acc: 0.1499\n",
      "Epoch 27/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.7353 - acc: 0.1535\n",
      "Epoch 28/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.7352 - acc: 0.1546\n",
      "Epoch 29/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.6837 - acc: 0.1591\n",
      "Epoch 30/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.6034 - acc: 0.1653\n",
      "Epoch 31/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.5413 - acc: 0.1699\n",
      "Epoch 32/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.4809 - acc: 0.1746\n",
      "Epoch 33/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.4270 - acc: 0.1792\n",
      "Epoch 34/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.3660 - acc: 0.1848\n",
      "Epoch 35/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.3128 - acc: 0.1903\n",
      "Epoch 36/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 4.2585 - acc: 0.1943\n",
      "Epoch 37/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 4.2054 - acc: 0.2005\n",
      "Epoch 38/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 4.1521 - acc: 0.2067\n",
      "Epoch 39/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 4.1055 - acc: 0.2116\n",
      "Epoch 40/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 4.0547 - acc: 0.2167\n",
      "Epoch 41/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 4.0030 - acc: 0.2222\n",
      "Epoch 42/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.9574 - acc: 0.2288\n",
      "Epoch 43/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.9131 - acc: 0.2342\n",
      "Epoch 44/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.8660 - acc: 0.2389\n",
      "Epoch 45/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.8203 - acc: 0.2466\n",
      "Epoch 46/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.7760 - acc: 0.2508\n",
      "Epoch 47/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.7302 - acc: 0.2565\n",
      "Epoch 48/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.6879 - acc: 0.2617\n",
      "Epoch 49/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.6465 - acc: 0.2671\n",
      "Epoch 50/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.7036 - acc: 0.2643\n",
      "Epoch 51/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.6918 - acc: 0.2661\n",
      "Epoch 52/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.6284 - acc: 0.2723\n",
      "Epoch 53/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.5465 - acc: 0.2814\n",
      "Epoch 54/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.4647 - acc: 0.2940\n",
      "Epoch 55/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.4183 - acc: 0.3001\n",
      "Epoch 56/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.3968 - acc: 0.3035\n",
      "Epoch 57/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.3517 - acc: 0.3103\n",
      "Epoch 58/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.3182 - acc: 0.3148\n",
      "Epoch 59/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.3823 - acc: 0.3080\n",
      "Epoch 60/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.3912 - acc: 0.3079\n",
      "Epoch 61/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.2914 - acc: 0.3229\n",
      "Epoch 62/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.2455 - acc: 0.3293\n",
      "Epoch 63/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.2199 - acc: 0.3335\n",
      "Epoch 64/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.1785 - acc: 0.3391\n",
      "Epoch 65/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.1578 - acc: 0.3422\n",
      "Epoch 66/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.1065 - acc: 0.3494\n",
      "Epoch 67/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.0766 - acc: 0.3543\n",
      "Epoch 68/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.0355 - acc: 0.3604\n",
      "Epoch 69/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.9861 - acc: 0.3685\n",
      "Epoch 70/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.9404 - acc: 0.3755\n",
      "Epoch 71/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.9207 - acc: 0.3791\n",
      "Epoch 72/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.8736 - acc: 0.3859\n",
      "Epoch 73/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.8555 - acc: 0.3900\n",
      "Epoch 74/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.8140 - acc: 0.3957\n",
      "Epoch 75/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.7744 - acc: 0.4017\n",
      "Epoch 76/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.7406 - acc: 0.4079\n",
      "Epoch 77/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.7099 - acc: 0.4135\n",
      "Epoch 78/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.6841 - acc: 0.4163\n",
      "Epoch 79/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.6573 - acc: 0.4224\n",
      "Epoch 80/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.6321 - acc: 0.4257\n",
      "Epoch 81/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.5920 - acc: 0.4334\n",
      "Epoch 82/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.5744 - acc: 0.4367\n",
      "Epoch 83/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.5427 - acc: 0.4421\n",
      "Epoch 84/100\n",
      "117185/117185 [==============================] - 133s 1ms/step - loss: 2.5149 - acc: 0.4480\n",
      "Epoch 85/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.4945 - acc: 0.4515\n",
      "Epoch 86/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.4580 - acc: 0.4572\n",
      "Epoch 87/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.4556 - acc: 0.4587\n",
      "Epoch 88/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.4028 - acc: 0.4675\n",
      "Epoch 89/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.3816 - acc: 0.4718\n",
      "Epoch 90/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.3483 - acc: 0.4788\n",
      "Epoch 91/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.4824 - acc: 0.4581\n",
      "Epoch 92/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.8107 - acc: 0.4244\n",
      "Epoch 93/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.9569 - acc: 0.4062\n",
      "Epoch 94/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.7670 - acc: 0.4201\n",
      "Epoch 95/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 3.0222 - acc: 0.3915\n",
      "Epoch 96/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 3.0211 - acc: 0.3968\n",
      "Epoch 97/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.9342 - acc: 0.4085\n",
      "Epoch 98/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.8236 - acc: 0.4215\n",
      "Epoch 99/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.7577 - acc: 0.4301\n",
      "Epoch 100/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.7880 - acc: 0.4268\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "in_filename = 'espnword.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work or maybe we can package some of these guys for a superstar so theres a little bit of hope there even if its misguided ridiculous and inane when i was there no hope whatsoever and that was my biggest mistakesimmons so you like what isiah has donelayden hell yeah take\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected bidirectional_1_input to have 3 dimensions, but got array with shape (1, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e4d9fa6f02e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# generate new text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e4d9fa6f02e0>\u001b[0m in \u001b[0;36mgenerate_seq\u001b[0;34m(model, tokenizer, seq_length, seed_text, n_words)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;31m# predict probabilities for each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0;31m# map predicted word index to word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mout_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \"\"\"\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected bidirectional_1_input to have 3 dimensions, but got array with shape (1, 50)"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'espnword.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('modelv2.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humping crisp defeats bomb and phillips jim importantly bout the best sports movies of alltime the biggest stretch in the draft that morrisons suit when flair flushed us he was involved to be the first of the way i understand what you can see the word upside in the first\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"tom\"\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'bidirectional_1_3/forward_lstm_1/kernel:0' shape=(10798, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_1_3/forward_lstm_1/recurrent_kernel:0' shape=(128, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_1_3/forward_lstm_1/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_1_3/backward_lstm_1/kernel:0' shape=(10798, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_1_3/backward_lstm_1/recurrent_kernel:0' shape=(128, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_1_3/backward_lstm_1/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1_3/kernel:0' shape=(256, 10798) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1_3/bias:0' shape=(10798,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('modelv2.h5')\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'aampm': 1,\n",
       " 'aaron': 2,\n",
       " 'aba': 3,\n",
       " 'abandoned': 4,\n",
       " 'abbie': 5,\n",
       " 'abc': 6,\n",
       " 'abcs': 7,\n",
       " 'abdurrahim': 8,\n",
       " 'aberration': 9,\n",
       " 'abilities': 10,\n",
       " 'ability': 11,\n",
       " 'able': 12,\n",
       " 'aboard': 13,\n",
       " 'about': 14,\n",
       " 'above': 15,\n",
       " 'aboveaverage': 16,\n",
       " 'abreu': 17,\n",
       " 'abruptly': 18,\n",
       " 'absence': 19,\n",
       " 'absolute': 20,\n",
       " 'absolutely': 21,\n",
       " 'absorb': 22,\n",
       " 'absorbed': 23,\n",
       " 'absorbing': 24,\n",
       " 'absurd': 25,\n",
       " 'abuse': 26,\n",
       " 'abysmal': 27,\n",
       " 'ac': 28,\n",
       " 'academy': 29,\n",
       " 'accent': 30,\n",
       " 'accents': 31,\n",
       " 'accept': 32,\n",
       " 'acceptable': 33,\n",
       " 'acceptance': 34,\n",
       " 'accepted': 35,\n",
       " 'accepting': 36,\n",
       " 'accepts': 37,\n",
       " 'access': 38,\n",
       " 'accident': 39,\n",
       " 'accidentally': 40,\n",
       " 'accidents': 41,\n",
       " 'accompanied': 42,\n",
       " 'accompanying': 43,\n",
       " 'accomplish': 44,\n",
       " 'accomplished': 45,\n",
       " 'accomplishment': 46,\n",
       " 'according': 47,\n",
       " 'accordingly': 48,\n",
       " 'account': 49,\n",
       " 'accountable': 50,\n",
       " 'accounts': 51,\n",
       " 'accuracy': 52,\n",
       " 'accurate': 53,\n",
       " 'accused': 54,\n",
       " 'ace': 55,\n",
       " 'aces': 56,\n",
       " 'achievement': 57,\n",
       " 'acid': 58,\n",
       " 'acie': 59,\n",
       " 'acknowledge': 60,\n",
       " 'acl': 61,\n",
       " 'acls': 62,\n",
       " 'acquire': 63,\n",
       " 'acquired': 64,\n",
       " 'acquiring': 65,\n",
       " 'across': 66,\n",
       " 'act': 67,\n",
       " 'acted': 68,\n",
       " 'acting': 69,\n",
       " 'action': 70,\n",
       " 'actions': 71,\n",
       " 'active': 72,\n",
       " 'actively': 73,\n",
       " 'activity': 74,\n",
       " 'actor': 75,\n",
       " 'actors': 76,\n",
       " 'actress': 77,\n",
       " 'actresses': 78,\n",
       " 'acts': 79,\n",
       " 'actual': 80,\n",
       " 'actually': 81,\n",
       " 'ad': 82,\n",
       " 'adam': 83,\n",
       " 'adams': 84,\n",
       " 'add': 85,\n",
       " 'addai': 86,\n",
       " 'added': 87,\n",
       " 'addicted': 88,\n",
       " 'adding': 89,\n",
       " 'addition': 90,\n",
       " 'additional': 91,\n",
       " 'additions': 92,\n",
       " 'address': 93,\n",
       " 'addressed': 94,\n",
       " 'adds': 95,\n",
       " 'adebisi': 96,\n",
       " 'adelman': 97,\n",
       " 'adelmans': 98,\n",
       " 'adequate': 99,\n",
       " 'adequately': 100,\n",
       " 'adjective': 101,\n",
       " 'adjust': 102,\n",
       " 'adjusted': 103,\n",
       " 'adjusting': 104,\n",
       " 'adjustment': 105,\n",
       " 'adjustments': 106,\n",
       " 'adlib': 107,\n",
       " 'admirable': 108,\n",
       " 'admire': 109,\n",
       " 'admirer': 110,\n",
       " 'admit': 111,\n",
       " 'admits': 112,\n",
       " 'admitted': 113,\n",
       " 'admittedly': 114,\n",
       " 'admitting': 115,\n",
       " 'ado': 116,\n",
       " 'adopt': 117,\n",
       " 'adopted': 118,\n",
       " 'adorable': 119,\n",
       " 'adrenaline': 120,\n",
       " 'adrian': 121,\n",
       " 'ads': 122,\n",
       " 'adult': 123,\n",
       " 'adults': 124,\n",
       " 'advance': 125,\n",
       " 'advanced': 126,\n",
       " 'advantage': 127,\n",
       " 'advertising': 128,\n",
       " 'advice': 129,\n",
       " 'advisor': 130,\n",
       " 'advocate': 131,\n",
       " 'afar': 132,\n",
       " 'afc': 133,\n",
       " 'affair': 134,\n",
       " 'affect': 135,\n",
       " 'affected': 136,\n",
       " 'affecting': 137,\n",
       " 'affection': 138,\n",
       " 'affects': 139,\n",
       " 'affleck': 140,\n",
       " 'afflecks': 141,\n",
       " 'afford': 142,\n",
       " 'afghanistan': 143,\n",
       " 'aforementioned': 144,\n",
       " 'aformentioned': 145,\n",
       " 'afraid': 146,\n",
       " 'african': 147,\n",
       " 'africanamerican': 148,\n",
       " 'africanamericans': 149,\n",
       " 'afro': 150,\n",
       " 'after': 151,\n",
       " 'afternoon': 152,\n",
       " 'afterthought': 153,\n",
       " 'afterward': 154,\n",
       " 'afterwards': 155,\n",
       " 'again': 156,\n",
       " 'against': 157,\n",
       " 'age': 158,\n",
       " 'aged': 159,\n",
       " 'agency': 160,\n",
       " 'agent': 161,\n",
       " 'agents': 162,\n",
       " 'ages': 163,\n",
       " 'aging': 164,\n",
       " 'ago': 165,\n",
       " 'agonized': 166,\n",
       " 'agonizing': 167,\n",
       " 'agony': 168,\n",
       " 'agoos': 169,\n",
       " 'agree': 170,\n",
       " 'agreed': 171,\n",
       " 'agreeing': 172,\n",
       " 'agreement': 173,\n",
       " 'agrees': 174,\n",
       " 'aguilera': 175,\n",
       " 'ah': 176,\n",
       " 'ahead': 177,\n",
       " 'ahmad': 178,\n",
       " 'ai': 179,\n",
       " 'aid': 180,\n",
       " 'aids': 181,\n",
       " 'aiken': 182,\n",
       " 'aikman': 183,\n",
       " 'ainge': 184,\n",
       " 'ainges': 185,\n",
       " 'aint': 186,\n",
       " 'air': 187,\n",
       " 'airball': 188,\n",
       " 'airballs': 189,\n",
       " 'aired': 190,\n",
       " 'airing': 191,\n",
       " 'airline': 192,\n",
       " 'airlines': 193,\n",
       " 'airplane': 194,\n",
       " 'airport': 195,\n",
       " 'airs': 196,\n",
       " 'aj': 197,\n",
       " 'aka': 198,\n",
       " 'al': 199,\n",
       " 'alan': 200,\n",
       " 'alarm': 201,\n",
       " 'alarming': 202,\n",
       " 'alas': 203,\n",
       " 'alaska': 204,\n",
       " 'alba': 205,\n",
       " 'albany': 206,\n",
       " 'albert': 207,\n",
       " 'album': 208,\n",
       " 'albums': 209,\n",
       " 'alcindor': 210,\n",
       " 'alcohol': 211,\n",
       " 'alcoholic': 212,\n",
       " 'alcs': 213,\n",
       " 'aldridge': 214,\n",
       " 'alec': 215,\n",
       " 'alert': 216,\n",
       " 'alerted': 217,\n",
       " 'alex': 218,\n",
       " 'alexander': 219,\n",
       " 'alexei': 220,\n",
       " 'alfonso': 221,\n",
       " 'ali': 222,\n",
       " 'alicia': 223,\n",
       " 'alien': 224,\n",
       " 'alike': 225,\n",
       " 'alis': 226,\n",
       " 'alive': 227,\n",
       " 'all': 228,\n",
       " 'allan': 229,\n",
       " 'allaround': 230,\n",
       " 'alleged': 231,\n",
       " 'allegedly': 232,\n",
       " 'allen': 233,\n",
       " 'alley': 234,\n",
       " 'alleyoop': 235,\n",
       " 'alleyoops': 236,\n",
       " 'alliance': 237,\n",
       " 'allies': 238,\n",
       " 'allin': 239,\n",
       " 'allnba': 240,\n",
       " 'allnbas': 241,\n",
       " 'allow': 242,\n",
       " 'allowed': 243,\n",
       " 'allowing': 244,\n",
       " 'allows': 245,\n",
       " 'allpro': 246,\n",
       " 'allstar': 247,\n",
       " 'allstars': 248,\n",
       " 'alltime': 249,\n",
       " 'allugly': 250,\n",
       " 'almost': 251,\n",
       " 'alone': 252,\n",
       " 'along': 253,\n",
       " 'alongside': 254,\n",
       " 'alonly': 255,\n",
       " 'alonzo': 256,\n",
       " 'alpha': 257,\n",
       " 'already': 258,\n",
       " 'alright': 259,\n",
       " 'also': 260,\n",
       " 'alsorans': 261,\n",
       " 'alter': 262,\n",
       " 'altercation': 263,\n",
       " 'alternate': 264,\n",
       " 'alternately': 265,\n",
       " 'alternative': 266,\n",
       " 'although': 267,\n",
       " 'altogether': 268,\n",
       " 'alton': 269,\n",
       " 'alvin': 270,\n",
       " 'always': 271,\n",
       " 'am': 272,\n",
       " 'amanda': 273,\n",
       " 'amare': 274,\n",
       " 'amas': 275,\n",
       " 'amateur': 276,\n",
       " 'amazed': 277,\n",
       " 'amazing': 278,\n",
       " 'amazingly': 279,\n",
       " 'amazon': 280,\n",
       " 'amazoncom': 281,\n",
       " 'ambassador': 282,\n",
       " 'amber': 283,\n",
       " 'ambiguous': 284,\n",
       " 'ambled': 285,\n",
       " 'amen': 286,\n",
       " 'america': 287,\n",
       " 'american': 288,\n",
       " 'americans': 289,\n",
       " 'americas': 290,\n",
       " 'ami': 291,\n",
       " 'aminus': 292,\n",
       " 'amok': 293,\n",
       " 'among': 294,\n",
       " 'amount': 295,\n",
       " 'amounted': 296,\n",
       " 'amounts': 297,\n",
       " 'amp': 298,\n",
       " 'amy': 299,\n",
       " 'an': 300,\n",
       " 'anaheim': 301,\n",
       " 'analogy': 302,\n",
       " 'analysis': 303,\n",
       " 'analyst': 304,\n",
       " 'analysts': 305,\n",
       " 'analyze': 306,\n",
       " 'analyzed': 307,\n",
       " 'anatomy': 308,\n",
       " 'anchor': 309,\n",
       " 'anchorman': 310,\n",
       " 'anchors': 311,\n",
       " 'and': 312,\n",
       " 'andersen': 313,\n",
       " 'anderson': 314,\n",
       " 'andersons': 315,\n",
       " 'andor': 316,\n",
       " 'andre': 317,\n",
       " 'andrea': 318,\n",
       " 'andrei': 319,\n",
       " 'andres': 320,\n",
       " 'andrew': 321,\n",
       " 'andrews': 322,\n",
       " 'andy': 323,\n",
       " 'anecdotes': 324,\n",
       " 'angeles': 325,\n",
       " 'angelessg': 326,\n",
       " 'angelina': 327,\n",
       " 'angelo': 328,\n",
       " 'angels': 329,\n",
       " 'angle': 330,\n",
       " 'angles': 331,\n",
       " 'angling': 332,\n",
       " 'angrily': 333,\n",
       " 'angry': 334,\n",
       " 'animal': 335,\n",
       " 'animals': 336,\n",
       " 'animated': 337,\n",
       " 'aniston': 338,\n",
       " 'ankle': 339,\n",
       " 'ankles': 340,\n",
       " 'ann': 341,\n",
       " 'anna': 342,\n",
       " 'anniversary': 343,\n",
       " 'announce': 344,\n",
       " 'announced': 345,\n",
       " 'announcement': 346,\n",
       " 'announcer': 347,\n",
       " 'announcers': 348,\n",
       " 'announces': 349,\n",
       " 'announcing': 350,\n",
       " 'annoy': 351,\n",
       " 'annoyed': 352,\n",
       " 'annoying': 353,\n",
       " 'annual': 354,\n",
       " 'anonymous': 355,\n",
       " 'another': 356,\n",
       " 'anquan': 357,\n",
       " 'answer': 358,\n",
       " 'answered': 359,\n",
       " 'answering': 360,\n",
       " 'answers': 361,\n",
       " 'antagonizing': 362,\n",
       " 'antawn': 363,\n",
       " 'anthem': 364,\n",
       " 'anthony': 365,\n",
       " 'antichrist': 366,\n",
       " 'antoine': 367,\n",
       " 'antonio': 368,\n",
       " 'antwan': 369,\n",
       " 'anxiety': 370,\n",
       " 'anxiously': 371,\n",
       " 'any': 372,\n",
       " 'anybody': 373,\n",
       " 'anymore': 374,\n",
       " 'anyone': 375,\n",
       " 'anyones': 376,\n",
       " 'anything': 377,\n",
       " 'anythings': 378,\n",
       " 'anytime': 379,\n",
       " 'anyway': 380,\n",
       " 'anywhere': 381,\n",
       " 'ap': 382,\n",
       " 'apart': 383,\n",
       " 'apartment': 384,\n",
       " 'apex': 385,\n",
       " 'apiece': 386,\n",
       " 'aplus': 387,\n",
       " 'apocalypse': 388,\n",
       " 'apollo': 389,\n",
       " 'apologies': 390,\n",
       " 'apologize': 391,\n",
       " 'apology': 392,\n",
       " 'appalled': 393,\n",
       " 'apparent': 394,\n",
       " 'apparently': 395,\n",
       " 'appeal': 396,\n",
       " 'appealing': 397,\n",
       " 'appear': 398,\n",
       " 'appearance': 399,\n",
       " 'appearances': 400,\n",
       " 'appeared': 401,\n",
       " 'appearing': 402,\n",
       " 'appears': 403,\n",
       " 'applaud': 404,\n",
       " 'applauded': 405,\n",
       " 'applauding': 406,\n",
       " 'applause': 407,\n",
       " 'apple': 408,\n",
       " 'applied': 409,\n",
       " 'applies': 410,\n",
       " 'apply': 411,\n",
       " 'applying': 412,\n",
       " 'appreciate': 413,\n",
       " 'appreciated': 414,\n",
       " 'appreciation': 415,\n",
       " 'apprentice': 416,\n",
       " 'approach': 417,\n",
       " 'approached': 418,\n",
       " 'approaches': 419,\n",
       " 'approaching': 420,\n",
       " 'appropriate': 421,\n",
       " 'approval': 422,\n",
       " 'approved': 423,\n",
       " 'approximately': 424,\n",
       " 'april': 425,\n",
       " 'araujo': 426,\n",
       " 'arbor': 427,\n",
       " 'arc': 428,\n",
       " 'archibald': 429,\n",
       " 'archie': 430,\n",
       " 'archive': 431,\n",
       " 'archives': 432,\n",
       " 'archrival': 433,\n",
       " 'arclight': 434,\n",
       " 'are': 435,\n",
       " 'area': 436,\n",
       " 'areas': 437,\n",
       " 'arena': 438,\n",
       " 'arenas': 439,\n",
       " 'arent': 440,\n",
       " 'argue': 441,\n",
       " 'argued': 442,\n",
       " 'arguing': 443,\n",
       " 'argument': 444,\n",
       " 'arguments': 445,\n",
       " 'ari': 446,\n",
       " 'ariza': 447,\n",
       " 'arizona': 448,\n",
       " 'arizsg': 449,\n",
       " 'arli': 450,\n",
       " 'arlington': 451,\n",
       " 'arm': 452,\n",
       " 'arms': 453,\n",
       " 'armstrong': 454,\n",
       " 'arnold': 455,\n",
       " 'arod': 456,\n",
       " 'around': 457,\n",
       " 'array': 458,\n",
       " 'arrest': 459,\n",
       " 'arrested': 460,\n",
       " 'arrive': 461,\n",
       " 'arrived': 462,\n",
       " 'arrives': 463,\n",
       " 'arrogant': 464,\n",
       " 'arroyo': 465,\n",
       " 'art': 466,\n",
       " 'artest': 467,\n",
       " 'artests': 468,\n",
       " 'arthur': 469,\n",
       " 'article': 470,\n",
       " 'articles': 471,\n",
       " 'articulate': 472,\n",
       " 'artificial': 473,\n",
       " 'artist': 474,\n",
       " 'as': 475,\n",
       " 'ashamed': 476,\n",
       " 'ashlee': 477,\n",
       " 'ashley': 478,\n",
       " 'ashton': 479,\n",
       " 'asian': 480,\n",
       " 'aside': 481,\n",
       " 'ask': 482,\n",
       " 'asked': 483,\n",
       " 'asking': 484,\n",
       " 'asks': 485,\n",
       " 'asleep': 486,\n",
       " 'aspect': 487,\n",
       " 'aspiring': 488,\n",
       " 'ass': 489,\n",
       " 'assassin': 490,\n",
       " 'assassination': 491,\n",
       " 'assembled': 492,\n",
       " 'assembly': 493,\n",
       " 'asses': 494,\n",
       " 'assessment': 495,\n",
       " 'asset': 496,\n",
       " 'assets': 497,\n",
       " 'assign': 498,\n",
       " 'assist': 499,\n",
       " 'assistant': 500,\n",
       " 'assistants': 501,\n",
       " 'assists': 502,\n",
       " 'association': 503,\n",
       " 'assume': 504,\n",
       " 'assumed': 505,\n",
       " 'assuming': 506,\n",
       " 'asterisk': 507,\n",
       " 'astin': 508,\n",
       " 'astonishing': 509,\n",
       " 'astounding': 510,\n",
       " 'astrodome': 511,\n",
       " 'astros': 512,\n",
       " 'at': 513,\n",
       " 'atbat': 514,\n",
       " 'atbats': 515,\n",
       " 'ate': 516,\n",
       " 'ateam': 517,\n",
       " 'athens': 518,\n",
       " 'athlete': 519,\n",
       " 'athletes': 520,\n",
       " 'athletic': 521,\n",
       " 'athleticism': 522,\n",
       " 'atkins': 523,\n",
       " 'atlanta': 524,\n",
       " 'atlantas': 525,\n",
       " 'atlantasg': 526,\n",
       " 'atlantic': 527,\n",
       " 'atm': 528,\n",
       " 'atmosphere': 529,\n",
       " 'atrocious': 530,\n",
       " 'attached': 531,\n",
       " 'attack': 532,\n",
       " 'attacked': 533,\n",
       " 'attacking': 534,\n",
       " 'attacks': 535,\n",
       " 'attempt': 536,\n",
       " 'attempted': 537,\n",
       " 'attempting': 538,\n",
       " 'attempts': 539,\n",
       " 'attend': 540,\n",
       " 'attendance': 541,\n",
       " 'attended': 542,\n",
       " 'attending': 543,\n",
       " 'attends': 544,\n",
       " 'attention': 545,\n",
       " 'attitude': 546,\n",
       " 'attorney': 547,\n",
       " 'attracted': 548,\n",
       " 'attraction': 549,\n",
       " 'attractive': 550,\n",
       " 'auction': 551,\n",
       " 'audible': 552,\n",
       " 'audibles': 553,\n",
       " 'audience': 554,\n",
       " 'audio': 555,\n",
       " 'audition': 556,\n",
       " 'auditioning': 557,\n",
       " 'auerbach': 558,\n",
       " 'auerbachs': 559,\n",
       " 'aug': 560,\n",
       " 'august': 561,\n",
       " 'augusta': 562,\n",
       " 'austin': 563,\n",
       " 'australian': 564,\n",
       " 'authentic': 565,\n",
       " 'author': 566,\n",
       " 'auto': 567,\n",
       " 'autobiography': 568,\n",
       " 'autographed': 569,\n",
       " 'automatic': 570,\n",
       " 'automatically': 571,\n",
       " 'autopsy': 572,\n",
       " 'available': 573,\n",
       " 'average': 574,\n",
       " 'averaged': 575,\n",
       " 'averages': 576,\n",
       " 'averaging': 577,\n",
       " 'avery': 578,\n",
       " 'avn': 579,\n",
       " 'avoid': 580,\n",
       " 'avoided': 581,\n",
       " 'avoiding': 582,\n",
       " 'awaiting': 583,\n",
       " 'awake': 584,\n",
       " 'award': 585,\n",
       " 'awarded': 586,\n",
       " 'awards': 587,\n",
       " 'aware': 588,\n",
       " 'away': 589,\n",
       " 'awesome': 590,\n",
       " 'awesomely': 591,\n",
       " 'awful': 592,\n",
       " 'awhile': 593,\n",
       " 'awkward': 594,\n",
       " 'awkwardly': 595,\n",
       " 'ax': 596,\n",
       " 'aykroyd': 597,\n",
       " 'b': 598,\n",
       " 'babbling': 599,\n",
       " 'babcock': 600,\n",
       " 'babe': 601,\n",
       " 'babes': 602,\n",
       " 'babies': 603,\n",
       " 'baby': 604,\n",
       " 'babys': 605,\n",
       " 'bachelor': 606,\n",
       " 'bachelorette': 607,\n",
       " 'back': 608,\n",
       " 'backboard': 609,\n",
       " 'backbreaking': 610,\n",
       " 'backcourt': 611,\n",
       " 'backed': 612,\n",
       " 'backers': 613,\n",
       " 'backfield': 614,\n",
       " 'background': 615,\n",
       " 'backgroundimage': 616,\n",
       " 'backgroundrepeat': 617,\n",
       " 'backhanded': 618,\n",
       " 'backing': 619,\n",
       " 'backlash': 620,\n",
       " 'backs': 621,\n",
       " 'backstage': 622,\n",
       " 'backtoback': 623,\n",
       " 'backup': 624,\n",
       " 'backward': 625,\n",
       " 'backwards': 626,\n",
       " 'backyard': 627,\n",
       " 'bacon': 628,\n",
       " 'bad': 629,\n",
       " 'bada': 630,\n",
       " 'badly': 631,\n",
       " 'bag': 632,\n",
       " 'baggage': 633,\n",
       " 'baggy': 634,\n",
       " 'bags': 635,\n",
       " 'bailed': 636,\n",
       " 'bailey': 637,\n",
       " 'baileys': 638,\n",
       " 'baio': 639,\n",
       " 'bait': 640,\n",
       " 'baked': 641,\n",
       " 'baker': 642,\n",
       " 'balance': 643,\n",
       " 'balanced': 644,\n",
       " 'balboa': 645,\n",
       " 'balboas': 646,\n",
       " 'balcony': 647,\n",
       " 'bald': 648,\n",
       " 'baldelli': 649,\n",
       " 'balding': 650,\n",
       " 'baldwin': 651,\n",
       " 'balkman': 652,\n",
       " 'ball': 653,\n",
       " 'ballboy': 654,\n",
       " 'ballistic': 655,\n",
       " 'ballot': 656,\n",
       " 'ballpark': 657,\n",
       " 'ballparks': 658,\n",
       " 'balls': 659,\n",
       " 'baltimore': 660,\n",
       " 'baltimores': 661,\n",
       " 'bam': 662,\n",
       " 'ban': 663,\n",
       " 'banana': 664,\n",
       " 'band': 665,\n",
       " 'banded': 666,\n",
       " 'bands': 667,\n",
       " 'bandwagon': 668,\n",
       " 'bang': 669,\n",
       " 'banged': 670,\n",
       " 'bangedup': 671,\n",
       " 'banging': 672,\n",
       " 'bangs': 673,\n",
       " 'banished': 674,\n",
       " 'bank': 675,\n",
       " 'banked': 676,\n",
       " 'banker': 677,\n",
       " 'banking': 678,\n",
       " 'bankrupt': 679,\n",
       " 'banks': 680,\n",
       " 'banned': 681,\n",
       " 'banner': 682,\n",
       " 'bar': 683,\n",
       " 'barajas': 684,\n",
       " 'barbara': 685,\n",
       " 'barbaro': 686,\n",
       " 'barbecue': 687,\n",
       " 'barber': 688,\n",
       " 'barbosa': 689,\n",
       " 'bare': 690,\n",
       " 'barely': 691,\n",
       " 'bargain': 692,\n",
       " 'bargaining': 693,\n",
       " 'bargnani': 694,\n",
       " 'barista': 695,\n",
       " 'barkin': 696,\n",
       " 'barking': 697,\n",
       " 'barkley': 698,\n",
       " 'barnes': 699,\n",
       " 'baron': 700,\n",
       " 'baroni': 701,\n",
       " 'barrage': 702,\n",
       " 'barrera': 703,\n",
       " 'barrier': 704,\n",
       " 'barring': 705,\n",
       " 'barry': 706,\n",
       " 'barrymore': 707,\n",
       " 'bars': 708,\n",
       " 'bartender': 709,\n",
       " 'bartenders': 710,\n",
       " 'bartman': 711,\n",
       " 'barton': 712,\n",
       " 'base': 713,\n",
       " 'baseball': 714,\n",
       " 'baseballs': 715,\n",
       " 'based': 716,\n",
       " 'baseline': 717,\n",
       " 'baseman': 718,\n",
       " 'basement': 719,\n",
       " 'bases': 720,\n",
       " 'bash': 721,\n",
       " 'bashing': 722,\n",
       " 'basic': 723,\n",
       " 'basically': 724,\n",
       " 'basis': 725,\n",
       " 'basket': 726,\n",
       " 'basketball': 727,\n",
       " 'basketballreferencecom': 728,\n",
       " 'baskets': 729,\n",
       " 'bass': 730,\n",
       " 'bastard': 731,\n",
       " 'bat': 732,\n",
       " 'batch': 733,\n",
       " 'bateman': 734,\n",
       " 'bates': 735,\n",
       " 'bath': 736,\n",
       " 'bathroom': 737,\n",
       " 'batista': 738,\n",
       " 'batman': 739,\n",
       " 'bats': 740,\n",
       " 'batted': 741,\n",
       " 'batter': 742,\n",
       " 'battered': 743,\n",
       " 'batteries': 744,\n",
       " 'batters': 745,\n",
       " 'battier': 746,\n",
       " 'batting': 747,\n",
       " 'battle': 748,\n",
       " 'battled': 749,\n",
       " 'battles': 750,\n",
       " 'battling': 751,\n",
       " 'bauer': 752,\n",
       " 'bauers': 753,\n",
       " 'bavetta': 754,\n",
       " 'bawling': 755,\n",
       " 'bay': 756,\n",
       " 'bayless': 757,\n",
       " 'baylor': 758,\n",
       " 'bc': 759,\n",
       " 'bcs': 760,\n",
       " 'be': 761,\n",
       " 'beach': 762,\n",
       " 'beane': 763,\n",
       " 'bear': 764,\n",
       " 'beard': 765,\n",
       " 'beards': 766,\n",
       " 'bears': 767,\n",
       " 'beat': 768,\n",
       " 'beaten': 769,\n",
       " 'beating': 770,\n",
       " 'beats': 771,\n",
       " 'beauties': 772,\n",
       " 'beautiful': 773,\n",
       " 'beauty': 774,\n",
       " 'beaver': 775,\n",
       " 'became': 776,\n",
       " 'because': 777,\n",
       " 'beck': 778,\n",
       " 'beckett': 779,\n",
       " 'become': 780,\n",
       " 'becomes': 781,\n",
       " 'becoming': 782,\n",
       " 'bed': 783,\n",
       " 'bedroom': 784,\n",
       " 'bee': 785,\n",
       " 'beef': 786,\n",
       " 'beefy': 787,\n",
       " 'beek': 788,\n",
       " 'been': 789,\n",
       " 'beer': 790,\n",
       " 'beers': 791,\n",
       " 'bees': 792,\n",
       " 'before': 793,\n",
       " 'beforehand': 794,\n",
       " 'began': 795,\n",
       " 'begged': 796,\n",
       " 'begging': 797,\n",
       " 'begin': 798,\n",
       " 'beginning': 799,\n",
       " 'begins': 800,\n",
       " 'begrudgingly': 801,\n",
       " 'begs': 802,\n",
       " 'behalf': 803,\n",
       " 'behavior': 804,\n",
       " 'behind': 805,\n",
       " 'behindthescenes': 806,\n",
       " 'being': 807,\n",
       " 'beings': 808,\n",
       " 'belated': 809,\n",
       " 'belichick': 810,\n",
       " 'belichickbrady': 811,\n",
       " 'belichicks': 812,\n",
       " 'belief': 813,\n",
       " 'believable': 814,\n",
       " 'believe': 815,\n",
       " 'believed': 816,\n",
       " 'believer': 817,\n",
       " 'believes': 818,\n",
       " 'believing': 819,\n",
       " 'bell': 820,\n",
       " 'bellamy': 821,\n",
       " 'belligerent': 822,\n",
       " 'belly': 823,\n",
       " 'belong': 824,\n",
       " 'belonged': 825,\n",
       " 'belongs': 826,\n",
       " 'beloved': 827,\n",
       " 'below': 828,\n",
       " 'belowaverage': 829,\n",
       " 'belt': 830,\n",
       " 'belted': 831,\n",
       " 'belting': 832,\n",
       " 'beltran': 833,\n",
       " 'belts': 834,\n",
       " 'belushi': 835,\n",
       " 'bemoaning': 836,\n",
       " 'ben': 837,\n",
       " 'bench': 838,\n",
       " 'benchclearing': 839,\n",
       " 'benched': 840,\n",
       " 'benches': 841,\n",
       " 'bend': 842,\n",
       " 'bender': 843,\n",
       " 'benefactor': 844,\n",
       " 'benefit': 845,\n",
       " 'bengals': 846,\n",
       " 'bennett': 847,\n",
       " 'benoit': 848,\n",
       " 'bens': 849,\n",
       " 'benson': 850,\n",
       " 'berkman': 851,\n",
       " 'berle': 852,\n",
       " 'berman': 853,\n",
       " 'bernard': 854,\n",
       " 'bernie': 855,\n",
       " 'bernstein': 856,\n",
       " 'berra': 857,\n",
       " 'berry': 858,\n",
       " 'beside': 859,\n",
       " 'besides': 860,\n",
       " 'best': 861,\n",
       " 'bestcase': 862,\n",
       " 'bestlooking': 863,\n",
       " 'bestselling': 864,\n",
       " 'bet': 865,\n",
       " 'betrayed': 866,\n",
       " 'bets': 867,\n",
       " 'better': 868,\n",
       " 'betting': 869,\n",
       " 'bettis': 870,\n",
       " 'bettman': 871,\n",
       " 'between': 872,\n",
       " 'beverly': 873,\n",
       " 'beware': 874,\n",
       " 'beyonce': 875,\n",
       " 'beyond': 876,\n",
       " 'bias': 877,\n",
       " 'biased': 878,\n",
       " 'bibby': 879,\n",
       " 'bid': 880,\n",
       " 'bidder': 881,\n",
       " 'bidding': 882,\n",
       " 'biding': 883,\n",
       " 'bids': 884,\n",
       " 'big': 885,\n",
       " 'bigamy': 886,\n",
       " 'biggame': 887,\n",
       " 'bigger': 888,\n",
       " 'biggest': 889,\n",
       " 'biggie': 890,\n",
       " 'bigmarket': 891,\n",
       " 'bigmoney': 892,\n",
       " 'bigtime': 893,\n",
       " 'bilas': 894,\n",
       " 'bill': 895,\n",
       " 'billboard': 896,\n",
       " 'billick': 897,\n",
       " 'billie': 898,\n",
       " 'billion': 899,\n",
       " 'billionaire': 900,\n",
       " 'bills': 901,\n",
       " 'billups': 902,\n",
       " 'billy': 903,\n",
       " 'bilson': 904,\n",
       " 'bimbo': 905,\n",
       " 'bimbos': 906,\n",
       " 'bin': 907,\n",
       " 'bing': 908,\n",
       " 'binge': 909,\n",
       " 'binges': 910,\n",
       " 'bingo': 911,\n",
       " 'bird': 912,\n",
       " 'birdie': 913,\n",
       " 'birdmagic': 914,\n",
       " 'birdman': 915,\n",
       " 'birds': 916,\n",
       " 'birth': 917,\n",
       " 'birthday': 918,\n",
       " 'bish': 919,\n",
       " 'bit': 920,\n",
       " 'bitch': 921,\n",
       " 'bitching': 922,\n",
       " 'bite': 923,\n",
       " 'bites': 924,\n",
       " 'biting': 925,\n",
       " 'bits': 926,\n",
       " 'bitter': 927,\n",
       " 'bitterness': 928,\n",
       " 'bittersweet': 929,\n",
       " 'bizarre': 930,\n",
       " 'bizarro': 931,\n",
       " 'bj': 932,\n",
       " 'black': 933,\n",
       " 'blackberry': 934,\n",
       " 'blacked': 935,\n",
       " 'blackjack': 936,\n",
       " 'blackman': 937,\n",
       " 'blacks': 938,\n",
       " 'bladder': 939,\n",
       " 'blah': 940,\n",
       " 'blair': 941,\n",
       " 'blake': 942,\n",
       " 'blame': 943,\n",
       " 'blamed': 944,\n",
       " 'blaming': 945,\n",
       " 'bland': 946,\n",
       " 'blank': 947,\n",
       " 'blanket': 948,\n",
       " 'blaring': 949,\n",
       " 'blast': 950,\n",
       " 'blatant': 951,\n",
       " 'blatantly': 952,\n",
       " 'blaylock': 953,\n",
       " 'blazer': 954,\n",
       " 'blazers': 955,\n",
       " 'bleachers': 956,\n",
       " 'bleak': 957,\n",
       " 'bledsoe': 958,\n",
       " 'bledsoes': 959,\n",
       " 'bleeding': 960,\n",
       " 'bleep': 961,\n",
       " 'bleeping': 962,\n",
       " 'blend': 963,\n",
       " 'bless': 964,\n",
       " 'blessed': 965,\n",
       " 'blew': 966,\n",
       " 'blind': 967,\n",
       " 'blink': 968,\n",
       " 'blinking': 969,\n",
       " 'blitz': 970,\n",
       " 'bloated': 971,\n",
       " 'block': 972,\n",
       " 'blockbuster': 973,\n",
       " 'blocked': 974,\n",
       " 'blocking': 975,\n",
       " 'blocks': 976,\n",
       " 'blog': 977,\n",
       " 'bloggers': 978,\n",
       " 'blogs': 979,\n",
       " 'blond': 980,\n",
       " 'blonde': 981,\n",
       " 'blood': 982,\n",
       " 'bloody': 983,\n",
       " 'bloopers': 984,\n",
       " 'blount': 985,\n",
       " 'blow': 986,\n",
       " 'blowing': 987,\n",
       " 'blown': 988,\n",
       " 'blowout': 989,\n",
       " 'blowouts': 990,\n",
       " 'blows': 991,\n",
       " 'blue': 992,\n",
       " 'blueboy': 993,\n",
       " 'bluechipper': 994,\n",
       " 'bluechippers': 995,\n",
       " 'blues': 996,\n",
       " 'blurb': 997,\n",
       " 'bminus': 998,\n",
       " 'bo': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-aefc63a9b630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "model.predict(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pr', 'people', 'running', 'the', 'events', 'hope', 'i', 'might', 'write', 'about', 'them', 'not', 'because', 'im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes']\n",
      "['people', 'running', 'the', 'events', 'hope', 'i', 'might', 'write', 'about', 'them', 'not', 'because', 'im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes']\n",
      "['running', 'the', 'events', 'hope', 'i', 'might', 'write', 'about', 'them', 'not', 'because', 'im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt']\n",
      "['the', 'events', 'hope', 'i', 'might', 'write', 'about', 'them', 'not', 'because', 'im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get']\n",
      "['events', 'hope', 'i', 'might', 'write', 'about', 'them', 'not', 'because', 'im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back']\n",
      "['hope', 'i', 'might', 'write', 'about', 'them', 'not', 'because', 'im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to']\n",
      "['i', 'might', 'write', 'about', 'them', 'not', 'because', 'im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my']\n",
      "['might', 'write', 'about', 'them', 'not', 'because', 'im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my']\n",
      "['write', 'about', 'them', 'not', 'because', 'im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very']\n",
      "['about', 'them', 'not', 'because', 'im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main']\n",
      "['them', 'not', 'because', 'im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer']\n",
      "['not', 'because', 'im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if']\n",
      "['because', 'im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i']\n",
      "['im', 'actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent']\n",
      "['actually', 'a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had']\n",
      "['a', 'celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the']\n",
      "['celebrity', 'i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i']\n",
      "['i', 'left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got']\n",
      "['left', 'that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show']\n",
      "['that', 'morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on']\n",
      "['morning', 'dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when']\n",
      "['dressed', 'in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they']\n",
      "['in', 'casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix']\n",
      "['casual', 'clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the']\n",
      "['clothes', 'couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same']\n",
      "['couldnt', 'get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things']\n",
      "['get', 'back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god']\n",
      "['back', 'to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the']\n",
      "['to', 'my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest']\n",
      "['my', 'my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of']\n",
      "['my', 'very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the']\n",
      "['very', 'main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time']\n",
      "['main', 'homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and']\n",
      "['homer', 'if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar']\n",
      "['if', 'i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along']\n",
      "['i', 'havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with']\n",
      "['havent', 'had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the']\n",
      "['had', 'the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams']\n",
      "['the', 'i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that']\n",
      "['i', 'got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that', 'holt']\n",
      "['got', 'show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that', 'holt', 'couldnt']\n",
      "['show', 'on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that', 'holt', 'couldnt', 'have']\n",
      "['on', 'when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that', 'holt', 'couldnt', 'have', 'considered']\n",
      "['when', 'they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that', 'holt', 'couldnt', 'have', 'considered', 'to']\n",
      "['they', 'mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that', 'holt', 'couldnt', 'have', 'considered', 'to', 'understand']\n",
      "['mix', 'the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that', 'holt', 'couldnt', 'have', 'considered', 'to', 'understand', 'as']\n",
      "['the', 'same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that', 'holt', 'couldnt', 'have', 'considered', 'to', 'understand', 'as', 'much']\n",
      "['same', 'things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that', 'holt', 'couldnt', 'have', 'considered', 'to', 'understand', 'as', 'much', 'as']\n",
      "['things', 'god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that', 'holt', 'couldnt', 'have', 'considered', 'to', 'understand', 'as', 'much', 'as', 'the']\n",
      "['god', 'the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that', 'holt', 'couldnt', 'have', 'considered', 'to', 'understand', 'as', 'much', 'as', 'the', 'patriots']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that', 'holt', 'couldnt', 'have', 'considered', 'to', 'understand', 'as', 'much', 'as', 'the', 'patriots', 'is']\n",
      "['the', 'rest', 'of', 'the', 'time', 'and', 'bar', 'along', 'with', 'the', 'teams', 'that', 'holt', 'couldnt', 'have', 'considered', 'to', 'understand', 'as', 'much', 'as', 'the', 'patriots', 'is', 'how']\n",
      "['arent', 'with', 'their', 'careers', 'or', 'ever', 'puts', 'it', 'it', 'on', 'de', 'there', 'could', 'be', 'an', 'absolute', 'assassin', 'if', 'there', 'does', 'any', 'reason', 'why', 'this', 'is']\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LEN = 25\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "model = load_model('modelv2.h5')\n",
    "\n",
    "seed_index = np.random.randint(len(sentences+sentences_test))\n",
    "seed = (sentences+sentences_test)[seed_index]\n",
    "#seed = ['the', 'best', 'player', 'in', 'the', 'world', 'is']\n",
    "\n",
    "\n",
    "'''\n",
    "for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "\n",
    "        \n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t, word_indices[word]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "        \n",
    "        print(seed)    \n",
    "        print(sentence)\n",
    "'''\n",
    "\n",
    "diversity = 1\n",
    "sentence = seed\n",
    "\n",
    "print(seed)  \n",
    "for i in range(50):\n",
    "    x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "    for t, word in enumerate(sentence):\n",
    "        x_pred[0, t, word_indices[word]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, diversity)\n",
    "    next_word = indices_word[next_index]\n",
    "\n",
    "    sentence = sentence[1:]\n",
    "    print(sentence)\n",
    "    sentence.append(next_word)\n",
    "    \n",
    "print(sentence)\n",
    " \n",
    "for i in range(50):\n",
    "    x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "    for t, word in enumerate(sentence):\n",
    "        x_pred[0, t, word_indices[word]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, diversity)\n",
    "    next_word = indices_word[next_index]\n",
    "\n",
    "    sentence = sentence[1:]\n",
    "    sentence.append(next_word)\n",
    "    \n",
    "\n",
    "  \n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
