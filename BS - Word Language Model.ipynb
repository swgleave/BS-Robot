{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the rumors are true and Doc Rivers gets a contract extension soon, I just hope the Celtics tell the truth in the ensuing news conference. Don't feed us lines like \"Thanks to Doc, our young players \n",
      "['if', 'the', 'rumors', 'are', 'true', 'and', 'doc', 'rivers', 'gets', 'a', 'contract', 'extension', 'soon', 'i', 'just', 'hope', 'the', 'celtics', 'tell', 'the', 'truth', 'in', 'the', 'ensuing', 'news', 'conference', 'dont', 'feed', 'us', 'lines', 'like', 'thanks', 'to', 'doc', 'our', 'young', 'players', 'made', 'major', 'strides', 'this', 'season', 'or', 'we', 'would', 'have', 'made', 'the', 'playoffs', 'if', 'we', 'didnt', 'have', 'so', 'many', 'injuries', 'thats', 'a', 'load', 'of', 'crap', 'maybe', 'those', 'arent', 'lies', 'but', 'they', 'seem', 'like', 'fibs', 'along', 'the', 'lines', 'of', 'i', 'dont', 'think', 'those', 'jeans', 'make', 'you', 'look', 'fat', 'at', 'all', 'theyre', 'just', 'the', 'wrong', 'size', 'and', 'i', 'wasnt', 'checking', 'her', 'out', 'i', 'thought', 'i', 'recognized', 'her', 'am', 'i', 'complaining', 'hell', 'no', 'you', 'couldnt', 'have', 'asked', 'for', 'a', 'better', 'leader', 'for', 'an', 'undercover', 'tanking', 'mission', 'this', 'was', 'like', 'hiring', 'eddie', 'griffin', 'to', 'wreck', 'your', 'archenemys', 'ferrari', 'so', 'why', 'not', 'admit', 'this', 'at', 'the', 'news', 'conference', 'and', 'if', 'youre', 'not', 'admitting', 'it', 'have', 'some', 'fun', 'and', 'hand', 'out', 'a', 'top', 'reasons', 'why', 'were', 'extending', 'doc', 'rivers', 'that', 'have', 'nothing', 'to', 'do', 'with', 'the', 'fact', 'that', 'he', 'successfully', 'executed', 'his', 'tanking', 'mission', 'list', 'reason', 'no', 'its', 'not', 'every', 'day', 'you', 'can', 'lock', 'up', 'a', 'coach', 'with', 'a', 'career', 'record', 'of', 'no', 'we', 'appreciate', 'how', 'doc', 'made', 'the', 'best']\n",
      "Total Tokens: 1016491\n",
      "Unique Tokens: 39395\n",
      "Total Sequences: 1016440\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# load document\n",
    "in_filename = 'cleanedespn.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    "\n",
    "#grab subset\n",
    "#print(len(doc))\n",
    "#doc = doc[:700000]\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'espnword.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 50)            629350    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12587)             1271287   \n",
      "=================================================================\n",
      "Total params: 2,051,537\n",
      "Trainable params: 2,051,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "117185/117185 [==============================] - 147s 1ms/step - loss: 7.2820 - acc: 0.0551\n",
      "Epoch 2/100\n",
      "117185/117185 [==============================] - 143s 1ms/step - loss: 6.9235 - acc: 0.0572\n",
      "Epoch 3/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 6.7480 - acc: 0.0650\n",
      "Epoch 4/100\n",
      "117185/117185 [==============================] - 142s 1ms/step - loss: 6.5891 - acc: 0.0718\n",
      "Epoch 5/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 6.4318 - acc: 0.0813\n",
      "Epoch 6/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 6.3011 - acc: 0.0875\n",
      "Epoch 7/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 6.1881 - acc: 0.0916\n",
      "Epoch 8/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 6.0917 - acc: 0.0951\n",
      "Epoch 9/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 6.0211 - acc: 0.0974\n",
      "Epoch 10/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 5.9298 - acc: 0.1023\n",
      "Epoch 11/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.8445 - acc: 0.1060\n",
      "Epoch 12/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.7634 - acc: 0.1105\n",
      "Epoch 13/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.6868 - acc: 0.1134\n",
      "Epoch 14/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.5987 - acc: 0.1169\n",
      "Epoch 15/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.5244 - acc: 0.1192\n",
      "Epoch 16/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.4380 - acc: 0.1219\n",
      "Epoch 17/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.3655 - acc: 0.1258\n",
      "Epoch 18/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.2915 - acc: 0.1281\n",
      "Epoch 19/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 5.2111 - acc: 0.1305\n",
      "Epoch 20/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.1355 - acc: 0.1330\n",
      "Epoch 21/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 5.0621 - acc: 0.1359\n",
      "Epoch 22/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.9934 - acc: 0.1388\n",
      "Epoch 23/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.9412 - acc: 0.1416\n",
      "Epoch 24/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.9384 - acc: 0.1412\n",
      "Epoch 25/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.8568 - acc: 0.1454\n",
      "Epoch 26/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.7777 - acc: 0.1499\n",
      "Epoch 27/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.7353 - acc: 0.1535\n",
      "Epoch 28/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.7352 - acc: 0.1546\n",
      "Epoch 29/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.6837 - acc: 0.1591\n",
      "Epoch 30/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.6034 - acc: 0.1653\n",
      "Epoch 31/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.5413 - acc: 0.1699\n",
      "Epoch 32/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.4809 - acc: 0.1746\n",
      "Epoch 33/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.4270 - acc: 0.1792\n",
      "Epoch 34/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.3660 - acc: 0.1848\n",
      "Epoch 35/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 4.3128 - acc: 0.1903\n",
      "Epoch 36/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 4.2585 - acc: 0.1943\n",
      "Epoch 37/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 4.2054 - acc: 0.2005\n",
      "Epoch 38/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 4.1521 - acc: 0.2067\n",
      "Epoch 39/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 4.1055 - acc: 0.2116\n",
      "Epoch 40/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 4.0547 - acc: 0.2167\n",
      "Epoch 41/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 4.0030 - acc: 0.2222\n",
      "Epoch 42/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.9574 - acc: 0.2288\n",
      "Epoch 43/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.9131 - acc: 0.2342\n",
      "Epoch 44/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.8660 - acc: 0.2389\n",
      "Epoch 45/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.8203 - acc: 0.2466\n",
      "Epoch 46/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.7760 - acc: 0.2508\n",
      "Epoch 47/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.7302 - acc: 0.2565\n",
      "Epoch 48/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.6879 - acc: 0.2617\n",
      "Epoch 49/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.6465 - acc: 0.2671\n",
      "Epoch 50/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.7036 - acc: 0.2643\n",
      "Epoch 51/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.6918 - acc: 0.2661\n",
      "Epoch 52/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.6284 - acc: 0.2723\n",
      "Epoch 53/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.5465 - acc: 0.2814\n",
      "Epoch 54/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.4647 - acc: 0.2940\n",
      "Epoch 55/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.4183 - acc: 0.3001\n",
      "Epoch 56/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.3968 - acc: 0.3035\n",
      "Epoch 57/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.3517 - acc: 0.3103\n",
      "Epoch 58/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.3182 - acc: 0.3148\n",
      "Epoch 59/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.3823 - acc: 0.3080\n",
      "Epoch 60/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.3912 - acc: 0.3079\n",
      "Epoch 61/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.2914 - acc: 0.3229\n",
      "Epoch 62/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.2455 - acc: 0.3293\n",
      "Epoch 63/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.2199 - acc: 0.3335\n",
      "Epoch 64/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.1785 - acc: 0.3391\n",
      "Epoch 65/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.1578 - acc: 0.3422\n",
      "Epoch 66/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.1065 - acc: 0.3494\n",
      "Epoch 67/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 3.0766 - acc: 0.3543\n",
      "Epoch 68/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 3.0355 - acc: 0.3604\n",
      "Epoch 69/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.9861 - acc: 0.3685\n",
      "Epoch 70/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.9404 - acc: 0.3755\n",
      "Epoch 71/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.9207 - acc: 0.3791\n",
      "Epoch 72/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.8736 - acc: 0.3859\n",
      "Epoch 73/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.8555 - acc: 0.3900\n",
      "Epoch 74/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.8140 - acc: 0.3957\n",
      "Epoch 75/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.7744 - acc: 0.4017\n",
      "Epoch 76/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.7406 - acc: 0.4079\n",
      "Epoch 77/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.7099 - acc: 0.4135\n",
      "Epoch 78/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.6841 - acc: 0.4163\n",
      "Epoch 79/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.6573 - acc: 0.4224\n",
      "Epoch 80/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.6321 - acc: 0.4257\n",
      "Epoch 81/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.5920 - acc: 0.4334\n",
      "Epoch 82/100\n",
      "117185/117185 [==============================] - 141s 1ms/step - loss: 2.5744 - acc: 0.4367\n",
      "Epoch 83/100\n",
      "117185/117185 [==============================] - 140s 1ms/step - loss: 2.5427 - acc: 0.4421\n",
      "Epoch 84/100\n",
      "117185/117185 [==============================] - 133s 1ms/step - loss: 2.5149 - acc: 0.4480\n",
      "Epoch 85/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.4945 - acc: 0.4515\n",
      "Epoch 86/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.4580 - acc: 0.4572\n",
      "Epoch 87/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.4556 - acc: 0.4587\n",
      "Epoch 88/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.4028 - acc: 0.4675\n",
      "Epoch 89/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.3816 - acc: 0.4718\n",
      "Epoch 90/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.3483 - acc: 0.4788\n",
      "Epoch 91/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.4824 - acc: 0.4581\n",
      "Epoch 92/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.8107 - acc: 0.4244\n",
      "Epoch 93/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.9569 - acc: 0.4062\n",
      "Epoch 94/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.7670 - acc: 0.4201\n",
      "Epoch 95/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 3.0222 - acc: 0.3915\n",
      "Epoch 96/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 3.0211 - acc: 0.3968\n",
      "Epoch 97/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.9342 - acc: 0.4085\n",
      "Epoch 98/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.8236 - acc: 0.4215\n",
      "Epoch 99/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.7577 - acc: 0.4301\n",
      "Epoch 100/100\n",
      "117185/117185 [==============================] - 128s 1ms/step - loss: 2.7880 - acc: 0.4268\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "in_filename = 'espnword.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "respect yet the fight scenes are better than ever its the first ever sequel that blatantly unapologetically steals moments from previous movies in the series yet theres something endearing about the way it keeps happening i felt terrible for sly after all this movie is a desperate transparent attempt to stay\n",
      "\n",
      "relevant mcgwire and sully because he could act after the same logic that resembles the past few weeks devouring the overall success guard gets thrown out for the bucks i mean everythingas we found out to be thrown out stealing seconds to break me and i think the hell was\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'espnword.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the worst idea in the time i dont think hes performing karaorocky theres a precedent here game added to the farm c you can find out of the first time in the field that willie deserves to exonerate a hint of potential the strategy just a little desperate for\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"dog\"\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
